import{o,c as r,k as a,q as n,s as _,A as s,e as t}from"../modules/vue-BxaKmS3U.js";import{I as l}from"./default-DecF49pF.js";import{_ as p,ac as e}from"../index-B5wOFWXg.js";import{p as c,u as i,f as m}from"./context-CxHehx3A.js";import"../modules/shiki-CX1VuG5-.js";const d=t("h3",null,"MLP",-1),u=t("p",null,"在transformer,自注意机制主要是混合不同元素之间的信息",-1),f=t("p",null,"而mlp才是真正信息的提炼",-1),h=t("p",null,[t("img",{src:"https://cdn.jsdelivr.net/gh/open17/Pic/img/202404231338148.png",alt:""})],-1),g=t("p",null,"对每个元素通过全连接层投影到原来4倍的大小,再通过Relu激活层投影回原始大小",-1),x=t("p",null,"全连接层的权重对每个元素共享",-1),P={__name:"7",setup(v){return c(e),i(),(k,B)=>(o(),r(l,n(_(s(m)(s(e),6))),{default:a(()=>[d,u,f,h,g,x]),_:1},16))}},q=p(P,[["__file","/@slidev/slides/7.md"]]);export{q as default};
